{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask-ml\n",
    "In this notebook we will play a little with `dask-ml` and see how we can use it together with tensorflow estimators to make our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# standard library\n",
    "import itertools\n",
    "import sys, os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "from collections import OrderedDict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask\n",
    "\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# dask-ml\n",
    "from dask_ml.preprocessing import StandardScaler, MinMaxScaler\n",
    "from dask_ml.linear_model import PartialSGDClassifier\n",
    "\n",
    "# tesnorflow \n",
    "import tensorflow as tf\n",
    "\n",
    "# local imports\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../src\"))\n",
    "\n",
    "# this styling is purely my preference\n",
    "# less chartjunk\n",
    "sns.set_context('notebook', font_scale=1.5, rc={'line.linewidth': 2.5})\n",
    "sns.set(style='ticks', palette='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.context.set_options at 0x7f084ef28828>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask.set_options(temporary_directory='/home/jovyan/work/partd/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>sibling_pos</th>\n",
       "      <th>no_classes</th>\n",
       "      <th>id_len</th>\n",
       "      <th>class_len</th>\n",
       "      <th>no_children</th>\n",
       "      <th>text_len</th>\n",
       "      <th>descendant1_no_nodes</th>\n",
       "      <th>descendant1_no_children_avg</th>\n",
       "      <th>descendant1_id_len_avg</th>\n",
       "      <th>...</th>\n",
       "      <th>ancestor5_tag_h3</th>\n",
       "      <th>ancestor5_tag_maxamineignore</th>\n",
       "      <th>ancestor5_tag_a</th>\n",
       "      <th>ancestor5_tag_ifcommentsaccepted</th>\n",
       "      <th>ancestor5_tag_noindex</th>\n",
       "      <th>ancestor5_tag_property</th>\n",
       "      <th>ancestor5_tag_iframe</th>\n",
       "      <th>ancestor5_tag_http:</th>\n",
       "      <th>ancestor5_tag_bodyonload</th>\n",
       "      <th>content_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1327 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   depth  sibling_pos  no_classes  id_len  class_len  no_children  text_len  \\\n",
       "0      3           21           0       0          0            0         0   \n",
       "1      6            0           0      21          0            2        31   \n",
       "2      8           19           0       0          0            0        16   \n",
       "3      5           18           0       0          0            0        11   \n",
       "4      3           35           0       0          0            0         0   \n",
       "\n",
       "   descendant1_no_nodes  descendant1_no_children_avg  descendant1_id_len_avg  \\\n",
       "0                     0                          0.0                     0.0   \n",
       "1                     2                          0.0                     0.0   \n",
       "2                     0                          0.0                     0.0   \n",
       "3                     0                          0.0                     0.0   \n",
       "4                     0                          0.0                     0.0   \n",
       "\n",
       "       ...        ancestor5_tag_h3  ancestor5_tag_maxamineignore  \\\n",
       "0      ...                       0                             0   \n",
       "1      ...                       0                             0   \n",
       "2      ...                       0                             0   \n",
       "3      ...                       0                             0   \n",
       "4      ...                       0                             0   \n",
       "\n",
       "   ancestor5_tag_a  ancestor5_tag_ifcommentsaccepted  ancestor5_tag_noindex  \\\n",
       "0                0                                 0                      0   \n",
       "1                0                                 0                      0   \n",
       "2                0                                 0                      0   \n",
       "3                0                                 0                      0   \n",
       "4                0                                 0                      0   \n",
       "\n",
       "   ancestor5_tag_property  ancestor5_tag_iframe  ancestor5_tag_http:  \\\n",
       "0                       0                     0                    0   \n",
       "1                       0                     0                    0   \n",
       "2                       0                     0                    0   \n",
       "3                       0                     0                    0   \n",
       "4                       0                     0                    0   \n",
       "\n",
       "   ancestor5_tag_bodyonload  content_label  \n",
       "0                         0          False  \n",
       "1                         0          False  \n",
       "2                         0           True  \n",
       "3                         0          False  \n",
       "4                         0          False  \n",
       "\n",
       "[5 rows x 1327 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the features \n",
    "data_ddf = dd.read_csv('../data/final/dragnet/dom-full-01.csv')\n",
    "data_ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "First we are going to split the data into records and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the data\n",
    "X_da = data_ddf.drop(['url', 'path', 'content_label'], axis=1).values\n",
    "y_da = data_ddf['content_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(columns=None, copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()  # instantiate a scaler\n",
    "scaler.fit(X_da)  # fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "scaled_X_da = scaler.transform(X_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Now having scaled the data, we will try to feed it to a logistic regressor and see how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = y_da.mean().compute()\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# instantiate the classifier\n",
    "model = PartialSGDClassifier(fit_intercept=False, class_weight={0: class_weight, 1:1-class_weight}, classes=[0,1], max_iter=500)\n",
    "model.fit(scaled_X_da, y_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = tf.contrib.learn.infer_real_valued_columns_from_input(data_ddf.drop(['url', 'path', 'content_label'], axis=1).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.contrib.learn.LinearClassifier(\n",
    "    feature_columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_estimator = tf.contrib.learn.SKCompat(estimator)\n",
    "sk_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# save load_iris() sklearn dataset to iris\n",
    "# if you'd like to check dataset type use: type(load_iris())\n",
    "# if you'd like to view list of attributes use: dir(load_iris())\n",
    "iris = load_iris()\n",
    "\n",
    "# np.c_ is the numpy concatenate function\n",
    "# which is used to concat iris['data'] and iris['target'] arrays \n",
    "# for pandas column argument: concat iris['feature_names'] list\n",
    "# and string list (in this case one string); you can make this anything you'd like..  \n",
    "# the original dataset would probably call this ['Species']\n",
    "data1 = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "data1.columns = ['slen', 'swid', 'plen', 'pwid', 'target']\n",
    "data1_ddf = dd.from_pandas(data1, npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = tf.contrib.learn.infer_real_valued_columns_from_input_fn(df_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.contrib.learn.LinearClassifier(\n",
    "    feature_columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_estimator = tf.contrib.learn.SKCompat(estimator)\n",
    "estimator.fit(input_fn=df_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_estimator.fit(x=data1_ddf.drop('target', axis=1).compute(), y=data1_ddf['target'].compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ddf = data1_ddf.drop(['target'], axis=1).compute()\n",
    "y_ddf = data1_ddf['target'].compute()\n",
    "ddf_input_fn = tf.estimator.inputs.pandas_input_fn(x=x_ddf, y=y_ddf, queue_capacity=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data1_ddf.to_delayed()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data_ddf.to_delayed()[0]\n",
    "a.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ddf = data_ddf.drop(['url', 'path', 'content_label'], axis=1)\n",
    "scaled_ddf = (data_ddf - data_ddf.mean()) / data_ddf.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_delay = scaled_ddf.to_delayed()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_delay.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_ddf.to_delayed()[1].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.contrib.learn.extract_dask_data(data_ddf[['depth', 'sibling_pos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "  dict(\n",
    "      a=list(\"aabbcc\"), b=list(range(6))),\n",
    "  index=pd.date_range(\n",
    "      start=\"20100101\", periods=6))\n",
    "ddf = dd.from_pandas(df, npartitions=3)\n",
    "tf.contrib.learn.extract_dask_data(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.contrib.learn.extract_dask_data(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out the tensorflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()  # initialize the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the filenames queue\n",
    "filenames = tf.matching_files('../data/final/cleaneval/dom-full-*.csv')\n",
    "filenames_queue =  tf.train.string_input_producer(filenames, capacity=2)\n",
    "filenames_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = tf.TextLineReader(skip_header_lines=1)  # intialize the reader\n",
    "key, value = reader.read(filenames_queue)\n",
    "\n",
    "# inspect them\n",
    "key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the queue runners\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "# test the output\n",
    "print(sess.run(key))\n",
    "print(sess.run(value)) \n",
    "coord.request_stop()\n",
    "coord.join(threads)  # join it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv('../data/final/cleaneval/dom-full-*.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_vals = [['' if dtype.type() is None else dtype.type()] for dtype in ddf.dtypes]\n",
    "default_vals[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv_decoder(input_tensor, dtypes, **kwargs):\n",
    "    \"\"\"Raturns a csv_decoded tensor from the input_tensor. Requires a sample\n",
    "    file to determine the types. Also automatically converts booleans\"\"\"\n",
    "    \n",
    "    # infer the types\n",
    "    default_values = ['' if dtype.name in ['bool', 'object'] else dtype.type() for dtype in dtypes] #  convert bools and objs to string\n",
    "    default_values = map(lambda x: 0.0 if np.issubdtype(type(x), np.integer) else x, default_values) # convert ints to float\n",
    "    default_values = [[x] for x in default_values]  # must be wrapped in a list\n",
    "    decoded_tensors = tf.decode_csv(input_tensor, default_values, **kwargs)\n",
    "    \n",
    "    # replace bools with their conversions\n",
    "    for i, dtype in zip(range(len(decoded_tensors)), dtypes):\n",
    "        if dtype.name == 'bool':\n",
    "            condition = tf.equal(decoded_tensors[i], tf.constant('True'))\n",
    "            decoded_tensors[i] = tf.where(condition, tf.constant(1.0), tf.constant(0.0)) \n",
    "            \n",
    "    return decoded_tensors\n",
    "\n",
    "make_csv_decoder(value, dd.read_csv('../data/final/cleaneval/dom-full-*.csv').dtypes)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv_col_tensors(csv_pattern=None, csv_files=None, shuffle=True, num_epochs=10, csv_decoder_kwargs={}):\n",
    "    \"\"\"Returns a dict of column names and their corresponding tensors.\n",
    "    `shuffle` specifies whether the files and lines should be shuffled.\n",
    "    `num_epochs` specifies how many time to yield every file\"\"\"\n",
    "    # read all the files fitting the specification\n",
    "    if csv_pattern is not None:\n",
    "        filenames = tf.matching_files(csv_pattern)\n",
    "    elif csv_files is not None:\n",
    "        filenames = tf.train.string_input_producer(filenames) # hardcoded files\n",
    "    else:\n",
    "        # rais eif no file pattern specified\n",
    "        raise ValueError('either csv_files or csv_pattern has to be specified')\n",
    "        \n",
    "    filenames_queue =  tf.train.string_input_producer(filenames, shuffle=shuffle, num_epochs=num_epochs)\n",
    "\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)  # intialize the reader\n",
    "    key, value = reader.read(filenames_queue)\n",
    "    \n",
    "    # read the metadata\n",
    "    ddf = dd.read_csv(csv_pattern)\n",
    "    decoded_tensors = make_csv_decoder(value, ddf.dtypes, **csv_decoder_kwargs)\n",
    "\n",
    "    return {k:v for k,v in zip(ddf.columns, decoded_tensors)}\n",
    "\n",
    "tens_dict = make_csv_col_tensors('../data/final/cleaneval/dom-full-*.csv')\n",
    "tens_dict['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_features_and_labels(col_dict, feature_cols, label_cols):\n",
    "    \"\"\"\"Receives a dict of tensors and returns 2 packed ones from the feature \n",
    "    subest and lable subset\"\"\"\n",
    "    feature_tensors = [tens for key, tens in col_dict.items() if key in feature_cols]\n",
    "    label_tensors = [tens for key, tens in col_dict.items() if key in label_cols]\n",
    "    \n",
    "    # stack them\n",
    "    return tf.stack(feature_tensors), tf.stack(label_tensors)\n",
    "\n",
    "features, labels = pack_features_and_labels(tens_dict, ['depth', 'sibling_pos', 'text_len', 'descendant2_id_len_avg'], ['content_label'])\n",
    "features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle it\n",
    "data_batch, label_batch = tf.train.shuffle_batch([features, labels], batch_size=100, capacity=2000, min_after_dequeue=1000)\n",
    "data_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # variables must be initialized otherwise it fails \n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    # test the output\n",
    "    print(sess.run(data_batch))\n",
    "    print(sess.run(label_batch)) \n",
    "    \n",
    "    # again\n",
    "    print(sess.run(data_batch))\n",
    "    print(sess.run(label_batch)) \n",
    "\n",
    "    # finish\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
